{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adends/neural-network-with-embeddings-for-house-prices?scriptVersionId=169635138\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.impute import KNNImputer\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nplt.style.use('fivethirtyeight')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-31T22:33:08.771507Z","iopub.execute_input":"2024-03-31T22:33:08.771999Z","iopub.status.idle":"2024-03-31T22:33:08.787287Z","shell.execute_reply.started":"2024-03-31T22:33:08.771958Z","shell.execute_reply":"2024-03-31T22:33:08.786078Z"}}},{"cell_type":"markdown","source":"# House Prices - Advanced Regression Techniques\n## Introduction\n- This kaggle notebook will be for the kaggle competition **House Prices - Advanced Regression**. We will be exploring the dataset with visualizations and analysis. We will then preprocess it for our model. We will then create a PyTorch Neural Network with embedding layers for the categorical features.\n\n## Table of Contents\n- [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis)\n- [Data Preprocessing](#Data-Preprocessing)\n  - [Feature Engineering](#Feature-Engineering)\n  - [Encoding](#Encoding)\n  - [Feature Selection](#Feature-Selection)\n  - [Imputation](#Imputation)\n  - [Testing Dataset Preprocessing](#Testing-Dataset-Preprocessing)\n- [Model Building](#Model-Building)\n    - [Creating Tensors](#Creating-Tensors)\n    - [Embedding Sizes](#Embedding-Sizes)\n    - [Model Architecture](#Model-Architecture)\n    - [Model Evaluation and Submission](#Model-Evaluation-and-Submission)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:08.790274Z","iopub.execute_input":"2024-03-31T22:33:08.790668Z","iopub.status.idle":"2024-03-31T22:33:08.859127Z","shell.execute_reply.started":"2024-03-31T22:33:08.790638Z","shell.execute_reply":"2024-03-31T22:33:08.857862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:08.860367Z","iopub.execute_input":"2024-03-31T22:33:08.860695Z","iopub.status.idle":"2024-03-31T22:33:08.886421Z","shell.execute_reply.started":"2024-03-31T22:33:08.860667Z","shell.execute_reply":"2024-03-31T22:33:08.88516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this head snippet, we can see that we have some columns (Alley, PoolQC, MiscFeature) likely have a high number of missing values. We'll be sure to investigate those.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:08.887673Z","iopub.execute_input":"2024-03-31T22:33:08.88837Z","iopub.status.idle":"2024-03-31T22:33:08.913746Z","shell.execute_reply.started":"2024-03-31T22:33:08.888339Z","shell.execute_reply":"2024-03-31T22:33:08.912529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have quite a few columns (1600, **81**). A lot of these are probably irrelevant so we need to filter those out for later results. Attempts with all of these columns usually results in a model with an NaN loss, meaning no predictions can really be made. We also see we have quite a few categorical features. Our stratedgy for these will be embedding layers.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:08.917533Z","iopub.execute_input":"2024-03-31T22:33:08.91789Z","iopub.status.idle":"2024-03-31T22:33:09.025228Z","shell.execute_reply.started":"2024-03-31T22:33:08.917859Z","shell.execute_reply":"2024-03-31T22:33:09.024283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all columns that contain missing values and plot with barplot\nnon_full_cols = df.isna().sum()\nnon_full_cols = non_full_cols[non_full_cols != 0]\n\nfig, ax = plt.subplots(figsize=(10,5))\nnon_full_cols.plot(kind='bar', ax=ax, color='coral')\n\nax.set_title('Number of Missing Values per Column')\nax.set_xlabel('Columns')\nax.set_ylabel('Number of Missing Values')\n\nplt.xticks(rotation=45, ha='right') \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.026312Z","iopub.execute_input":"2024-03-31T22:33:09.026661Z","iopub.status.idle":"2024-03-31T22:33:09.48524Z","shell.execute_reply.started":"2024-03-31T22:33:09.026632Z","shell.execute_reply":"2024-03-31T22:33:09.484158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we get a better look into the completeness of our dataset. We have a couple of extreme columns with nearly all missing values (Remember original dataset was 1600 entries). For these extremely incomplete rows we are going to remove them. For the gray area columns like `LotsFrontage` and `FirePlaceQu`, we will keep them for now.","metadata":{}},{"cell_type":"code","source":"# Identifying mostly complete columns (low_missing_cols) and splitting them into Categorical and Numerical\nlow_missing_cols = ['MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'GarageType','GarageFinish', 'GarageYrBlt', 'GarageQual', 'GarageCond']\nfiltered_df = df[low_missing_cols]\n\nnumerical_missing_cols = filtered_df.select_dtypes(include='number').columns.tolist()\ncategorical_missing_cols = filtered_df.select_dtypes(include='object').columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.486735Z","iopub.execute_input":"2024-03-31T22:33:09.488007Z","iopub.status.idle":"2024-03-31T22:33:09.497548Z","shell.execute_reply.started":"2024-03-31T22:33:09.487964Z","shell.execute_reply":"2024-03-31T22:33:09.496191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure the selected categorical columns are indeed categorical\ndf[categorical_missing_cols]","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.498858Z","iopub.execute_input":"2024-03-31T22:33:09.49974Z","iopub.status.idle":"2024-03-31T22:33:09.523081Z","shell.execute_reply.started":"2024-03-31T22:33:09.499709Z","shell.execute_reply":"2024-03-31T22:33:09.522134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify mostly incomplete columns (Missing values greater than half of total)\nmissing_counts = df.isna().sum()\ncols_with_high_missing = missing_counts[missing_counts > len(df) // 2].index\nhigh_missing_cols = df[cols_with_high_missing] \n\nhigh_missing_cols","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.524663Z","iopub.execute_input":"2024-03-31T22:33:09.525359Z","iopub.status.idle":"2024-03-31T22:33:09.553341Z","shell.execute_reply.started":"2024-03-31T22:33:09.52531Z","shell.execute_reply":"2024-03-31T22:33:09.552262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Drop the high missing cols from before aswell as Id (Identifiers not relevant)\ndf.drop(high_missing_cols, axis=1, inplace=True)\ndf_test.drop(high_missing_cols, axis=1, inplace=True)\ndf.drop(['Id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.55589Z","iopub.execute_input":"2024-03-31T22:33:09.556281Z","iopub.status.idle":"2024-03-31T22:33:09.568599Z","shell.execute_reply.started":"2024-03-31T22:33:09.556251Z","shell.execute_reply":"2024-03-31T22:33:09.567286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\nWith more domain knowledge you could engineer more features based on the existing ones. For this notebook we will keep it simple and are only transforming certain features.\n- Replace `YearBuilt` and `GarageYrBlt` with `Age` and `GarageYrBlt`\n- New age features are time differences between Today and Year Built for more domain relevance\n","metadata":{}},{"cell_type":"code","source":"df['Age']=datetime.datetime.now().year-df['YearBuilt']\ndf['GarageAge'] = datetime.datetime.now().year-df['GarageYrBlt']\ndf.drop(['YearBuilt','GarageYrBlt'], axis=1, inplace=True)\n\ndf_test['Age']=datetime.datetime.now().year-df_test['YearBuilt']\ndf_test['GarageAge'] = datetime.datetime.now().year-df_test['GarageYrBlt']\ndf_test.drop(['YearBuilt','GarageYrBlt'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.570364Z","iopub.execute_input":"2024-03-31T22:33:09.570799Z","iopub.status.idle":"2024-03-31T22:33:09.589495Z","shell.execute_reply.started":"2024-03-31T22:33:09.570759Z","shell.execute_reply":"2024-03-31T22:33:09.588022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting distribution for Target Variable (SalePrice)\nfig, ax = plt.subplots(figsize=(10,5))\n\ndf['SalePrice'].plot(kind='kde',ax=ax, color='coral',)\ndf['SalePrice'].hist(density=True, ax=ax, color='teal', alpha=0.5, bins=40)  \n\nax.set_xlabel('Sale Price (Target)')\nax.set_ylabel('Density')\nax.set_title('Sale Price Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:09.59101Z","iopub.execute_input":"2024-03-31T22:33:09.591359Z","iopub.status.idle":"2024-03-31T22:33:10.06581Z","shell.execute_reply.started":"2024-03-31T22:33:09.591331Z","shell.execute_reply":"2024-03-31T22:33:10.064585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this plot, we can observe that our target variable, Sale Price, is normally distributed. This indicates that most houses are priced around a central value, suggesting a similar range of sale prices across the dataset. The normal distribution also implies that there is not a significant amount of extreme variation or outliers in sale prices.","metadata":{}},{"cell_type":"code","source":"numeric_df = df.select_dtypes(include='number')\nnumeric_features = numeric_df.columns\nnumeric_df.hist(figsize=(16, 22), bins=40, color='coral')","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:10.066912Z","iopub.execute_input":"2024-03-31T22:33:10.067258Z","iopub.status.idle":"2024-03-31T22:33:19.552755Z","shell.execute_reply.started":"2024-03-31T22:33:10.067229Z","shell.execute_reply":"2024-03-31T22:33:19.551418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these plots we can see that of the numerical features we have the continous variables indicated by the more complete graphs (SalePrice, Age, GarageAge, GarageArea, etc) and discrete indicated by the sparse graphs (GarageCars, FirePlaces, KitchenAbvGr, etc).","metadata":{}},{"cell_type":"code","source":"cat_df = df.select_dtypes(include='object')\nunique_counts = cat_df.nunique()\n\nplt.figure(figsize=(10, 6))\nunique_counts.plot(kind='bar', color='teal')\n\nplt.title('Number of Unique Categories in Each Categorical Column')\nplt.xlabel('Categorical Columns')\nplt.ylabel('Number of Unique Categories')\nplt.xticks(rotation=45, ha='right') \n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:35.602807Z","iopub.execute_input":"2024-03-31T22:33:35.60322Z","iopub.status.idle":"2024-03-31T22:33:36.229007Z","shell.execute_reply.started":"2024-03-31T22:33:35.60317Z","shell.execute_reply":"2024-03-31T22:33:36.227812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot gives us the cardinality of our categorical features, most are in the 0-5 range however we have some higher ones like Neighborhood.","metadata":{}},{"cell_type":"code","source":"# cat_df = pd.DataFrame(cat_df['MSZoning'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding\nSince we are going to use an embedding layer we will be using LabelEncoder. Different encoders can be experiemented with however, this one works easiest.\n\n","metadata":{}},{"cell_type":"code","source":"cat_features = df.select_dtypes(include=['object']).columns  \nencoders = {col: LabelEncoder().fit(df[col]) for col in cat_features}\n\nfor col in cat_features:\n    # Fit the encoder including an extra class for unseen categories\n    unique_values = list(df[col].unique()) + ['unseen_category']\n    encoders[col] = LabelEncoder().fit(unique_values)\n    \n    # Transform training data\n    df[col] = encoders[col].transform(df[col])\n    \n    # Transform test data, replacing unseen categories with 'unseen_category'\n    df_test[col] = df_test[col].apply(lambda x: x if x in encoders[col].classes_ else 'unseen_category')\n    df_test[col] = encoders[col].transform(df_test[col])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:38.838462Z","iopub.execute_input":"2024-03-31T22:33:38.838885Z","iopub.status.idle":"2024-03-31T22:33:39.406708Z","shell.execute_reply.started":"2024-03-31T22:33:38.83885Z","shell.execute_reply":"2024-03-31T22:33:39.405451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputation\n\nFor this iteration of the notebook we will be imputing with KNNImputer, in previous iterations I used just a simple median and mode imputation and haven't seen much performance gain over.","metadata":{}},{"cell_type":"code","source":"cat_features = [col for col in df.columns if col not in numeric_features and col != 'SalePrice']\n\nX = df.drop(columns=['SalePrice'], axis=1)\nX_test = df_test\n\nimputer = KNNImputer()\n\nX_imputed = imputer.fit_transform(X)\nX_test_imputed = imputer.transform(X_test.drop(['Id'], axis=1))\n\ndf.iloc[:, df.columns != 'SalePrice'] = X_imputed\ndf_test.iloc[:, df_test.columns != 'Id'] = X_test_imputed\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:42.66677Z","iopub.execute_input":"2024-03-31T22:33:42.667156Z","iopub.status.idle":"2024-03-31T22:33:42.969575Z","shell.execute_reply.started":"2024-03-31T22:33:42.667127Z","shell.execute_reply":"2024-03-31T22:33:42.968412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\nWe will employ a Decision Trees model to evaluate feature importance, which assists in identifying the most influential variables. With the feature importances we can select which features should be selected for our model defined by some threshold. A common threshold is usually above $.01$ however this parameter can be tuned for different results.","metadata":{}},{"cell_type":"code","source":"# Split data, scale it, fit to model and get important features\nX = df.drop('SalePrice', axis=1)\ny = df['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel = DecisionTreeRegressor(random_state=42)  \n\nmodel.fit(X_train, y_train)\nimportances = model.feature_importances_\nfeature_names = X.columns\n\nfeature_importances = dict(zip(feature_names, importances))","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:44.774753Z","iopub.execute_input":"2024-03-31T22:33:44.775178Z","iopub.status.idle":"2024-03-31T22:33:44.841475Z","shell.execute_reply.started":"2024-03-31T22:33:44.775142Z","shell.execute_reply":"2024-03-31T22:33:44.840618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter out variables that have little influence\nimportance_threshold = 0.0\n\nimportant_features = {feature: importance for feature, importance in feature_importances.items() if importance >= importance_threshold}\n\ndf = df[list(important_features.keys())]\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:46.830657Z","iopub.execute_input":"2024-03-31T22:33:46.831054Z","iopub.status.idle":"2024-03-31T22:33:46.875196Z","shell.execute_reply.started":"2024-03-31T22:33:46.831025Z","shell.execute_reply":"2024-03-31T22:33:46.874033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_cat_features = [cat for cat in cat_features if cat in df.columns]\nfiltered_numeric_features = [num for num in numeric_features if num in df.columns] \nfiltered_features = filtered_cat_features + filtered_numeric_features","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:49.926833Z","iopub.execute_input":"2024-03-31T22:33:49.927784Z","iopub.status.idle":"2024-03-31T22:33:49.933314Z","shell.execute_reply.started":"2024-03-31T22:33:49.927744Z","shell.execute_reply":"2024-03-31T22:33:49.932048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test[filtered_features + ['Id']]","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:52.157548Z","iopub.execute_input":"2024-03-31T22:33:52.158014Z","iopub.status.idle":"2024-03-31T22:33:52.166726Z","shell.execute_reply.started":"2024-03-31T22:33:52.157979Z","shell.execute_reply":"2024-03-31T22:33:52.165594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cat_features = [col for col in filtered_features if col in cat_features]\ntest_numeric_features = [col for col in filtered_features if col not in cat_features]","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:54.078763Z","iopub.execute_input":"2024-03-31T22:33:54.079814Z","iopub.status.idle":"2024-03-31T22:33:54.085874Z","shell.execute_reply.started":"2024-03-31T22:33:54.079764Z","shell.execute_reply":"2024-03-31T22:33:54.084616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_cat_features, filtered_numeric_features","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:33:55.615037Z","iopub.execute_input":"2024-03-31T22:33:55.61561Z","iopub.status.idle":"2024-03-31T22:33:55.625314Z","shell.execute_reply.started":"2024-03-31T22:33:55.615461Z","shell.execute_reply":"2024-03-31T22:33:55.623882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cat_features, test_numeric_features","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:34:43.583344Z","iopub.execute_input":"2024-03-31T22:34:43.584399Z","iopub.status.idle":"2024-03-31T22:34:43.592946Z","shell.execute_reply.started":"2024-03-31T22:34:43.584361Z","shell.execute_reply":"2024-03-31T22:34:43.591708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(len(filtered_cat_features), len(filtered_numeric_features)), (len(test_cat_features), len(test_numeric_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['SalePrice'] = y","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:25.194466Z","iopub.execute_input":"2024-03-31T22:35:25.194844Z","iopub.status.idle":"2024-03-31T22:35:25.200964Z","shell.execute_reply.started":"2024-03-31T22:35:25.194816Z","shell.execute_reply":"2024-03-31T22:35:25.199277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building\n\nWe are using a PyTorch Dataset which will convert our features into tensors which can be easily indexed. This approach is more modular and provides efficient data loading. If you're not familiar with PyTorch datasets you need to define 3 basic functions as well as inherit the superclass Dataset. The reason we added the additional logic for the case when target is None is for our submission dataset where that column is omitted.\n\n## Creating Tensors\n- Convert dataframes into numpy arrays \n- Convert numpy arrays into Tensors\n    - Create tensors (train, test) for Numerical Columns (float)\n    - Create tensors (train, test) for Categorical Cumns (int64)\n    - Create tensor for Target Variable (SalePrice)\n","metadata":{}},{"cell_type":"code","source":"class HouseDataset(Dataset):\n    def __init__(self, df, numeric_features, categorical_features, target=None):\n        self.df = df\n        self.numeric = torch.tensor(np.stack([df[num].values for num in numeric_features], axis=1), dtype=torch.float)\n        self.categorical = torch.tensor(np.stack([df[cat].values for cat in categorical_features], axis=1), dtype=torch.int64)\n        \n        if target is not None:\n            self.target = torch.tensor(df[target].values, dtype=torch.float).reshape(-1, 1)\n        else:\n            self.target = None\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if self.target is not None:\n            return self.categorical[idx], self.numeric[idx],  self.target[idx]\n        else:\n            return self.categorical[idx], self.numeric[idx]","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:27.306717Z","iopub.execute_input":"2024-03-31T22:35:27.307151Z","iopub.status.idle":"2024-03-31T22:35:27.317158Z","shell.execute_reply.started":"2024-03-31T22:35:27.307117Z","shell.execute_reply":"2024-03-31T22:35:27.316253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, df_testing = train_test_split(df, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:29.439257Z","iopub.execute_input":"2024-03-31T22:35:29.440113Z","iopub.status.idle":"2024-03-31T22:35:29.44994Z","shell.execute_reply.started":"2024-03-31T22:35:29.440068Z","shell.execute_reply":"2024-03-31T22:35:29.44899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = HouseDataset(df, filtered_numeric_features, filtered_cat_features, 'SalePrice')\ntest_dataset = HouseDataset(df_testing, filtered_numeric_features, filtered_cat_features, 'SalePrice')\nsubmit_dataset = HouseDataset(df_test, test_numeric_features, test_cat_features)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:30.891013Z","iopub.execute_input":"2024-03-31T22:35:30.89165Z","iopub.status.idle":"2024-03-31T22:35:30.912884Z","shell.execute_reply.started":"2024-03-31T22:35:30.891614Z","shell.execute_reply":"2024-03-31T22:35:30.911683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding Sizes \nSince the train and test columns result in varying amount of unique values we will need to get the embedding sizes for both the train and test. This will change how we compute the final embeddings.","metadata":{}},{"cell_type":"code","source":"t_embedding_sizes = [len(df_test[col].unique()) for col in df_test[test_cat_features]]\nprint(t_embedding_sizes)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:33.211219Z","iopub.execute_input":"2024-03-31T22:35:33.211697Z","iopub.status.idle":"2024-03-31T22:35:33.223926Z","shell.execute_reply.started":"2024-03-31T22:35:33.211663Z","shell.execute_reply":"2024-03-31T22:35:33.22285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_sizes = [len(df[col].unique()) for col in df[filtered_cat_features]]\nprint(embedding_sizes)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:35.082875Z","iopub.execute_input":"2024-03-31T22:35:35.083309Z","iopub.status.idle":"2024-03-31T22:35:35.096894Z","shell.execute_reply.started":"2024-03-31T22:35:35.083256Z","shell.execute_reply":"2024-03-31T22:35:35.095601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are taking the max between the two different embeddings to ensure we don't have shape issues when training or predicting.","metadata":{}},{"cell_type":"code","source":"embedding_dimensions = [(max(size, t_size)+1, min(50, (max(size, t_size)+1) // 2)) for size, t_size in zip(embedding_sizes, t_embedding_sizes)]\nembedding_dimensions","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:49.213554Z","iopub.execute_input":"2024-03-31T22:35:49.213966Z","iopub.status.idle":"2024-03-31T22:35:49.225822Z","shell.execute_reply.started":"2024-03-31T22:35:49.213934Z","shell.execute_reply":"2024-03-31T22:35:49.224542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture\n\n### Initialization\n\nIn the initialization phase we are defining two types of layers\n- **Embedding Layer**\n    - We create a layer for each embedding dimension defined earlier\n    - Define Dropout Layer for the embeddings \n    - Define Batch Normalization layer for embeddings \n\n- **Linear Layers**\n    - We create a series of layers dependant on the layers number passed\n    - For each layers we define a fully-connected Linear Layer accompanied by ReLU activation function\n    - Define Dropout layer for current Linear Layer\n    - Define Batch Normalization for current Linear Layer\n    \n### Forward Passes\n\n- For the forward pass we give the model the categorical and continous features for that batch. The model first calculates the embeddings based on the categorical features and embedding layers defined earlier. The continous features are then passed into the Batch Normalization to Standardize them before being concatenated with the embedded categorical features. The concatenated batch is then sent through the series of Fully-Connected Linear Layers.","metadata":{}},{"cell_type":"code","source":"class RegressionModel(nn.Module):\n    def __init__(self, embedding_dimensions, num_continuous, out_size, layers, dropout_rate=0.4):\n        super().__init__()\n        self.embeddings = nn.ModuleList([nn.Embedding(input_dim, output_dim) for input_dim, output_dim in embedding_dimensions])\n        self.embeddings_dropout = nn.Dropout(dropout_rate)\n        self.bn_continuous = nn.BatchNorm1d(num_continuous)\n        \n        layers_list = []\n        num_embeddings = sum((output for _, output in embedding_dimensions))\n        total_in = num_embeddings + num_continuous\n        \n        for i, layer in enumerate(layers):\n            layers_list.append(nn.Linear(total_in, layer))\n            layers_list.append(nn.ReLU(inplace=True))\n            layers_list.append(nn.BatchNorm1d(layer))\n            curr_dropout_rate = dropout_rate * (len(layers) - i) / len(layers)\n            layers_list.append(nn.Dropout(curr_dropout_rate))\n            total_in = layer\n            \n        layers_list.append(nn.Linear(layers[-1], out_size))\n        self.layers = nn.Sequential(*layers_list)\n            \n    def forward(self, categorical, continuous):\n        embeddings = []\n        for i, e in enumerate(self.embeddings):\n            embeddings.append(e(categorical[:, i]))\n        x = torch.cat(embeddings, 1)\n        x = self.embeddings_dropout(x)\n        \n        cont = self.bn_continuous(continuous)\n        x = torch.cat([x, cont], 1)\n        x = self.layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:35:59.711702Z","iopub.execute_input":"2024-03-31T22:35:59.712763Z","iopub.status.idle":"2024-03-31T22:35:59.726683Z","shell.execute_reply.started":"2024-03-31T22:35:59.712712Z","shell.execute_reply":"2024-03-31T22:35:59.725379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RegressionModel(embedding_dimensions, len(filtered_numeric_features), 1, [250, 100,50], 0.4)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:36:03.645275Z","iopub.execute_input":"2024-03-31T22:36:03.645726Z","iopub.status.idle":"2024-03-31T22:36:03.662382Z","shell.execute_reply.started":"2024-03-31T22:36:03.645693Z","shell.execute_reply":"2024-03-31T22:36:03.66123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:36:05.629254Z","iopub.execute_input":"2024-03-31T22:36:05.632153Z","iopub.status.idle":"2024-03-31T22:36:05.640228Z","shell.execute_reply.started":"2024-03-31T22:36:05.632111Z","shell.execute_reply":"2024-03-31T22:36:05.639392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a nice representation of our model, we can see the embedding layers defined by the dimensions we created earlier. Then we can see the series of linear layers we defined.","metadata":{}},{"cell_type":"markdown","source":"## Model Training\n\nIn the following cell, we make a training loop with several components. The first one to mention is we are using KFold Cross Validation to find the best pair of folds for training and validation. We also are using early stopping here in order to stop the model once loss has stopped decreasing. We are also training over a large number of epochs with a fast learning rate, this is due to our scheduler. We have a ReduceLROnPlateau which will reduce the learning rate once the model stops improving which allows us to use a high learning rate. In addition we are using RMSE as our loss function. ","metadata":{}},{"cell_type":"code","source":"EPOCHS = 500\npatience = 5\nkfold = KFold(n_splits=5, shuffle=True)\n\nbest_overall_accuracy = 0.0 \nbest_overall_loss = float('inf')\nbest_model_state = None\n\n# Lists to track training and validation losses over all epochs and folds\nall_train_losses = []\nall_val_losses = []\n\n# Begin k-fold cross-validation.\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(df.index, df['SalePrice'])):\n    print(f\"Starting fold {fold+1}\")\n    df_train, df_val = df.iloc[train_idx], df.iloc[val_idx]\n    \n    train_dataset = HouseDataset(df_train, filtered_numeric_features, filtered_cat_features, 'SalePrice')\n    val_dataset = HouseDataset(df_val, filtered_numeric_features, filtered_cat_features, 'SalePrice')\n    \n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n    \n    model = RegressionModel(embedding_dimensions, len(filtered_numeric_features), 1, [100,50], 0.4)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.1)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n    loss_function = nn.MSELoss()\n\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    early_stopping_patience = 250\n\n    # Lists to track per-fold training and validation losses\n    fold_train_losses = []\n    fold_val_losses = []\n\n    # Begin training for a set number of epochs.\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss_epoch = 0\n        for cat, cont, y in train_loader:\n            cat, cont, y = cat.to(device), cont.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_pred = model(cat, cont)\n            loss = torch.sqrt(loss_function(y_pred, y))\n            loss.backward()\n            optimizer.step()\n            train_loss_epoch += loss.item()\n\n        train_loss_epoch /= len(train_loader)\n        fold_train_losses.append(train_loss_epoch)  # Track training loss for the current fold\n\n        model.eval()\n        val_loss_epoch = 0\n        with torch.no_grad():\n            for val_cat, val_cont, val_y in val_loader:\n                val_cat, val_cont, val_y = val_cat.to(device), val_cont.to(device), val_y.to(device)\n                val_y_pred = model(val_cat, val_cont)\n                val_loss = torch.sqrt(loss_function(val_y_pred, val_y))\n                val_loss_epoch += val_loss.item()\n\n        val_loss_epoch /= len(val_loader)\n        fold_val_losses.append(val_loss_epoch)  # Track validation loss for the current fold\n\n        if val_loss_epoch < best_val_loss:\n            best_val_loss = val_loss_epoch\n            epochs_no_improve = 0\n            best_model_state = model.state_dict().copy()\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= early_stopping_patience:\n                print(f\"Early stopping triggered. Stopping training at fold {fold+1}, epoch {epoch+1}\")\n                break\n\n        scheduler.step(val_loss_epoch)\n\n    # Append per-fold losses to the overall tracking lists\n    all_train_losses.append(fold_train_losses)\n    all_val_losses.append(fold_val_losses)\n\n    if best_val_loss < best_overall_loss:\n        best_overall_loss = best_val_loss\n        best_model_state = model.state_dict().copy()\n\ntorch.save(best_model_state, 'best_model.bin')\nprint(\"Best model saved with overall lowest validation loss.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:36:09.383892Z","iopub.execute_input":"2024-03-31T22:36:09.384359Z","iopub.status.idle":"2024-03-31T22:40:43.233226Z","shell.execute_reply.started":"2024-03-31T22:36:09.384324Z","shell.execute_reply":"2024-03-31T22:40:43.232066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the best model from the best fold\nbest_model_path = 'best_model.bin'\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.to(device) ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:41:57.367418Z","iopub.execute_input":"2024-03-31T22:41:57.367884Z","iopub.status.idle":"2024-03-31T22:41:57.387271Z","shell.execute_reply.started":"2024-03-31T22:41:57.367827Z","shell.execute_reply":"2024-03-31T22:41:57.385814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_with_nan(list_of_lists):\n    max_length = max(len(single_list) for single_list in list_of_lists)\n    return [single_list + [np.nan]*(max_length - len(single_list)) for single_list in list_of_lists]\n\npadded_train_losses = pad_with_nan(all_train_losses)\npadded_val_losses = pad_with_nan(all_val_losses)\n\npadded_train_losses_array = np.array(padded_train_losses)\npadded_val_losses_array = np.array(padded_val_losses)\n\naverage_train_losses = np.nanmean(padded_train_losses_array, axis=0)\naverage_val_losses = np.nanmean(padded_val_losses_array, axis=0)\n\nplt.figure(figsize=(10, 6))\nplt.plot(average_train_losses, label='Average Training Loss', color='teal')\nplt.plot(average_val_losses, label='Average Validation Loss', color='coral')\nplt.title('Average Training and Validation Losses Across Folds')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:42:01.107241Z","iopub.execute_input":"2024-03-31T22:42:01.107673Z","iopub.status.idle":"2024-03-31T22:42:01.491018Z","shell.execute_reply.started":"2024-03-31T22:42:01.107638Z","shell.execute_reply":"2024-03-31T22:42:01.489792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph shows that we slowly decreased both losses over a lot of epochs. Usually the graph slows down at around 2500 which will be displayed here. If the epochs is set low this graph will usually have a lot of variance which indicates our model has not converged yet.","metadata":{}},{"cell_type":"markdown","source":"## Model Evaluation and Submission","metadata":{}},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) \n\nmodel.eval()\n\ntest_loss = 0\nall_y_pred = []\nall_y_true = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        cat, cont, y_true = batch\n        cat, cont, y_true = cat.to(device), cont.to(device), y_true.to(device)\n        y_pred = model(cat, cont)\n\n        loss = torch.sqrt(loss_function(y_pred, y_true))\n        test_loss += loss.item()\n\n        all_y_pred.append(y_pred)\n        all_y_true.append(y_true)\n\nall_y_pred = torch.cat(all_y_pred, dim=0)\nall_y_true = torch.cat(all_y_true, dim=0)\n\ntest_loss /= len(test_loader)\nprint(f\"Test Loss: {test_loss}\")\n\nmse = mean_squared_error(all_y_true.cpu().numpy(), all_y_pred.cpu().numpy())\nrmse = np.sqrt(mse)\nr2 = r2_score(all_y_true.cpu().numpy(), all_y_pred.cpu().numpy())\n\nprint(f\"Root Mean Squared Error: {rmse}\")\nprint(f\"R-squared: {r2}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:42:08.272583Z","iopub.execute_input":"2024-03-31T22:42:08.272965Z","iopub.status.idle":"2024-03-31T22:42:08.297219Z","shell.execute_reply.started":"2024-03-31T22:42:08.272937Z","shell.execute_reply":"2024-03-31T22:42:08.296028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_loader = DataLoader(submit_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:42:10.107459Z","iopub.execute_input":"2024-03-31T22:42:10.107878Z","iopub.status.idle":"2024-03-31T22:42:10.113845Z","shell.execute_reply.started":"2024-03-31T22:42:10.107845Z","shell.execute_reply":"2024-03-31T22:42:10.112649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ntest_loss = 0\nall_y_pred = []\n\nwith torch.no_grad():\n    for cat, cont in submit_loader:\n        cat, cont = cat.to(device), cont.to(device)\n        y_pred = model(cat, cont)\n        all_y_pred.append(y_pred)\n        \nall_y_pred = torch.cat(all_y_pred).cpu().numpy()\n\ndf_submit = pd.DataFrame({\n    'Id': df_test['Id'],\n    'SalePrice': all_y_pred.flatten()  \n})\n\ndf_submit.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T22:42:11.892465Z","iopub.execute_input":"2024-03-31T22:42:11.892885Z","iopub.status.idle":"2024-03-31T22:42:11.95997Z","shell.execute_reply.started":"2024-03-31T22:42:11.892851Z","shell.execute_reply":"2024-03-31T22:42:11.958883Z"},"trusted":true},"execution_count":null,"outputs":[]}]}